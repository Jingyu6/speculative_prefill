output_dir="./local/outputs/efficiency/search"
mkdir -p $output_dir

python -m speculative_prefill.vllm_benchmarks.latency     --model "meta-llama/Meta-Llama-3.1-70B-Instruct"     --enforce-eager     --enable-chunked-prefill False     --tensor-parallel-size 8     --max_model_len 32832     --input-len 4096     --output-len 1     --batch-size 128     --num-iters-warmup 4     --num-iters 16 > $output_dir/baseline_bs128_sl4096.txt

SPEC_CONFIG_PATH=./configs/config_p1.yaml python -m speculative_prefill.vllm_benchmarks.latency     --model "meta-llama/Meta-Llama-3.1-70B-Instruct"     --spec-prefill     --spec-model "meta-llama/Meta-Llama-3.1-8B-Instruct"     --enforce-eager     --enable-chunked-prefill False     --tensor-parallel-size 8     --max_model_len 32832     --input-len 4096     --output-len 1     --batch-size 128     --num-iters-warmup 4     --num-iters 16 > $output_dir/spec_p1_bs128_sl4096.txt

SPEC_CONFIG_PATH=./configs/config_p3.yaml python -m speculative_prefill.vllm_benchmarks.latency     --model "meta-llama/Meta-Llama-3.1-70B-Instruct"     --spec-prefill     --spec-model "meta-llama/Meta-Llama-3.1-8B-Instruct"     --enforce-eager     --enable-chunked-prefill False     --tensor-parallel-size 8     --max_model_len 32832     --input-len 4096     --output-len 1     --batch-size 128     --num-iters-warmup 4     --num-iters 16 > $output_dir/spec_p3_bs128_sl4096.txt

SPEC_CONFIG_PATH=./configs/config_p5.yaml python -m speculative_prefill.vllm_benchmarks.latency     --model "meta-llama/Meta-Llama-3.1-70B-Instruct"     --spec-prefill     --spec-model "meta-llama/Meta-Llama-3.1-8B-Instruct"     --enforce-eager     --enable-chunked-prefill False     --tensor-parallel-size 8     --max_model_len 32832     --input-len 4096     --output-len 1     --batch-size 128     --num-iters-warmup 4     --num-iters 16 > $output_dir/spec_p5_bs128_sl4096.txt

SPEC_CONFIG_PATH=./configs/config_p7.yaml python -m speculative_prefill.vllm_benchmarks.latency     --model "meta-llama/Meta-Llama-3.1-70B-Instruct"     --spec-prefill     --spec-model "meta-llama/Meta-Llama-3.1-8B-Instruct"     --enforce-eager     --enable-chunked-prefill False     --tensor-parallel-size 8     --max_model_len 32832     --input-len 4096     --output-len 1     --batch-size 128     --num-iters-warmup 4     --num-iters 16 > $output_dir/spec_p7_bs128_sl4096.txt

SPEC_CONFIG_PATH=./configs/config_p1_full_lah8.yaml python -m speculative_prefill.vllm_benchmarks.latency     --model "meta-llama/Meta-Llama-3.1-70B-Instruct"     --spec-prefill     --spec-model "meta-llama/Meta-Llama-3.1-8B-Instruct"     --enforce-eager     --enable-chunked-prefill False     --tensor-parallel-size 8     --max_model_len 32832     --input-len 4096     --output-len 1     --batch-size 128     --num-iters-warmup 4     --num-iters 16 > $output_dir/spec_p1_full_lah8_bs128_sl4096.txt

SPEC_CONFIG_PATH=./configs/config_p3_full_lah8.yaml python -m speculative_prefill.vllm_benchmarks.latency     --model "meta-llama/Meta-Llama-3.1-70B-Instruct"     --spec-prefill     --spec-model "meta-llama/Meta-Llama-3.1-8B-Instruct"     --enforce-eager     --enable-chunked-prefill False     --tensor-parallel-size 8     --max_model_len 32832     --input-len 4096     --output-len 1     --batch-size 128     --num-iters-warmup 4     --num-iters 16 > $output_dir/spec_p3_full_lah8_bs128_sl4096.txt

SPEC_CONFIG_PATH=./configs/config_p5_full_lah8.yaml python -m speculative_prefill.vllm_benchmarks.latency     --model "meta-llama/Meta-Llama-3.1-70B-Instruct"     --spec-prefill     --spec-model "meta-llama/Meta-Llama-3.1-8B-Instruct"     --enforce-eager     --enable-chunked-prefill False     --tensor-parallel-size 8     --max_model_len 32832     --input-len 4096     --output-len 1     --batch-size 128     --num-iters-warmup 4     --num-iters 16 > $output_dir/spec_p5_full_lah8_bs128_sl4096.txt

SPEC_CONFIG_PATH=./configs/config_p7_full_lah8.yaml python -m speculative_prefill.vllm_benchmarks.latency     --model "meta-llama/Meta-Llama-3.1-70B-Instruct"     --spec-prefill     --spec-model "meta-llama/Meta-Llama-3.1-8B-Instruct"     --enforce-eager     --enable-chunked-prefill False     --tensor-parallel-size 8     --max_model_len 32832     --input-len 4096     --output-len 1     --batch-size 128     --num-iters-warmup 4     --num-iters 16 > $output_dir/spec_p7_full_lah8_bs128_sl4096.txt

python -m speculative_prefill.vllm_benchmarks.latency     --model "meta-llama/Meta-Llama-3.1-70B-Instruct"     --enforce-eager     --enable-chunked-prefill False     --tensor-parallel-size 8     --max_model_len 32832     --input-len 8192     --output-len 1     --batch-size 64     --num-iters-warmup 4     --num-iters 16 > $output_dir/baseline_bs64_sl8192.txt

SPEC_CONFIG_PATH=./configs/config_p1.yaml python -m speculative_prefill.vllm_benchmarks.latency     --model "meta-llama/Meta-Llama-3.1-70B-Instruct"     --spec-prefill     --spec-model "meta-llama/Meta-Llama-3.1-8B-Instruct"     --enforce-eager     --enable-chunked-prefill False     --tensor-parallel-size 8     --max_model_len 32832     --input-len 8192     --output-len 1     --batch-size 64     --num-iters-warmup 4     --num-iters 16 > $output_dir/spec_p1_bs64_sl8192.txt

SPEC_CONFIG_PATH=./configs/config_p3.yaml python -m speculative_prefill.vllm_benchmarks.latency     --model "meta-llama/Meta-Llama-3.1-70B-Instruct"     --spec-prefill     --spec-model "meta-llama/Meta-Llama-3.1-8B-Instruct"     --enforce-eager     --enable-chunked-prefill False     --tensor-parallel-size 8     --max_model_len 32832     --input-len 8192     --output-len 1     --batch-size 64     --num-iters-warmup 4     --num-iters 16 > $output_dir/spec_p3_bs64_sl8192.txt

SPEC_CONFIG_PATH=./configs/config_p5.yaml python -m speculative_prefill.vllm_benchmarks.latency     --model "meta-llama/Meta-Llama-3.1-70B-Instruct"     --spec-prefill     --spec-model "meta-llama/Meta-Llama-3.1-8B-Instruct"     --enforce-eager     --enable-chunked-prefill False     --tensor-parallel-size 8     --max_model_len 32832     --input-len 8192     --output-len 1     --batch-size 64     --num-iters-warmup 4     --num-iters 16 > $output_dir/spec_p5_bs64_sl8192.txt

SPEC_CONFIG_PATH=./configs/config_p7.yaml python -m speculative_prefill.vllm_benchmarks.latency     --model "meta-llama/Meta-Llama-3.1-70B-Instruct"     --spec-prefill     --spec-model "meta-llama/Meta-Llama-3.1-8B-Instruct"     --enforce-eager     --enable-chunked-prefill False     --tensor-parallel-size 8     --max_model_len 32832     --input-len 8192     --output-len 1     --batch-size 64     --num-iters-warmup 4     --num-iters 16 > $output_dir/spec_p7_bs64_sl8192.txt

SPEC_CONFIG_PATH=./configs/config_p1_full_lah8.yaml python -m speculative_prefill.vllm_benchmarks.latency     --model "meta-llama/Meta-Llama-3.1-70B-Instruct"     --spec-prefill     --spec-model "meta-llama/Meta-Llama-3.1-8B-Instruct"     --enforce-eager     --enable-chunked-prefill False     --tensor-parallel-size 8     --max_model_len 32832     --input-len 8192     --output-len 1     --batch-size 64     --num-iters-warmup 4     --num-iters 16 > $output_dir/spec_p1_full_lah8_bs64_sl8192.txt

SPEC_CONFIG_PATH=./configs/config_p3_full_lah8.yaml python -m speculative_prefill.vllm_benchmarks.latency     --model "meta-llama/Meta-Llama-3.1-70B-Instruct"     --spec-prefill     --spec-model "meta-llama/Meta-Llama-3.1-8B-Instruct"     --enforce-eager     --enable-chunked-prefill False     --tensor-parallel-size 8     --max_model_len 32832     --input-len 8192     --output-len 1     --batch-size 64     --num-iters-warmup 4     --num-iters 16 > $output_dir/spec_p3_full_lah8_bs64_sl8192.txt

SPEC_CONFIG_PATH=./configs/config_p5_full_lah8.yaml python -m speculative_prefill.vllm_benchmarks.latency     --model "meta-llama/Meta-Llama-3.1-70B-Instruct"     --spec-prefill     --spec-model "meta-llama/Meta-Llama-3.1-8B-Instruct"     --enforce-eager     --enable-chunked-prefill False     --tensor-parallel-size 8     --max_model_len 32832     --input-len 8192     --output-len 1     --batch-size 64     --num-iters-warmup 4     --num-iters 16 > $output_dir/spec_p5_full_lah8_bs64_sl8192.txt

SPEC_CONFIG_PATH=./configs/config_p7_full_lah8.yaml python -m speculative_prefill.vllm_benchmarks.latency     --model "meta-llama/Meta-Llama-3.1-70B-Instruct"     --spec-prefill     --spec-model "meta-llama/Meta-Llama-3.1-8B-Instruct"     --enforce-eager     --enable-chunked-prefill False     --tensor-parallel-size 8     --max_model_len 32832     --input-len 8192     --output-len 1     --batch-size 64     --num-iters-warmup 4     --num-iters 16 > $output_dir/spec_p7_full_lah8_bs64_sl8192.txt

python -m speculative_prefill.vllm_benchmarks.latency     --model "meta-llama/Meta-Llama-3.1-70B-Instruct"     --enforce-eager     --enable-chunked-prefill False     --tensor-parallel-size 8     --max_model_len 32832     --input-len 16384     --output-len 1     --batch-size 32     --num-iters-warmup 4     --num-iters 16 > $output_dir/baseline_bs32_sl16384.txt

SPEC_CONFIG_PATH=./configs/config_p1.yaml python -m speculative_prefill.vllm_benchmarks.latency     --model "meta-llama/Meta-Llama-3.1-70B-Instruct"     --spec-prefill     --spec-model "meta-llama/Meta-Llama-3.1-8B-Instruct"     --enforce-eager     --enable-chunked-prefill False     --tensor-parallel-size 8     --max_model_len 32832     --input-len 16384     --output-len 1     --batch-size 32     --num-iters-warmup 4     --num-iters 16 > $output_dir/spec_p1_bs32_sl16384.txt

SPEC_CONFIG_PATH=./configs/config_p3.yaml python -m speculative_prefill.vllm_benchmarks.latency     --model "meta-llama/Meta-Llama-3.1-70B-Instruct"     --spec-prefill     --spec-model "meta-llama/Meta-Llama-3.1-8B-Instruct"     --enforce-eager     --enable-chunked-prefill False     --tensor-parallel-size 8     --max_model_len 32832     --input-len 16384     --output-len 1     --batch-size 32     --num-iters-warmup 4     --num-iters 16 > $output_dir/spec_p3_bs32_sl16384.txt

SPEC_CONFIG_PATH=./configs/config_p5.yaml python -m speculative_prefill.vllm_benchmarks.latency     --model "meta-llama/Meta-Llama-3.1-70B-Instruct"     --spec-prefill     --spec-model "meta-llama/Meta-Llama-3.1-8B-Instruct"     --enforce-eager     --enable-chunked-prefill False     --tensor-parallel-size 8     --max_model_len 32832     --input-len 16384     --output-len 1     --batch-size 32     --num-iters-warmup 4     --num-iters 16 > $output_dir/spec_p5_bs32_sl16384.txt

SPEC_CONFIG_PATH=./configs/config_p7.yaml python -m speculative_prefill.vllm_benchmarks.latency     --model "meta-llama/Meta-Llama-3.1-70B-Instruct"     --spec-prefill     --spec-model "meta-llama/Meta-Llama-3.1-8B-Instruct"     --enforce-eager     --enable-chunked-prefill False     --tensor-parallel-size 8     --max_model_len 32832     --input-len 16384     --output-len 1     --batch-size 32     --num-iters-warmup 4     --num-iters 16 > $output_dir/spec_p7_bs32_sl16384.txt

SPEC_CONFIG_PATH=./configs/config_p1_full_lah8.yaml python -m speculative_prefill.vllm_benchmarks.latency     --model "meta-llama/Meta-Llama-3.1-70B-Instruct"     --spec-prefill     --spec-model "meta-llama/Meta-Llama-3.1-8B-Instruct"     --enforce-eager     --enable-chunked-prefill False     --tensor-parallel-size 8     --max_model_len 32832     --input-len 16384     --output-len 1     --batch-size 32     --num-iters-warmup 4     --num-iters 16 > $output_dir/spec_p1_full_lah8_bs32_sl16384.txt

SPEC_CONFIG_PATH=./configs/config_p3_full_lah8.yaml python -m speculative_prefill.vllm_benchmarks.latency     --model "meta-llama/Meta-Llama-3.1-70B-Instruct"     --spec-prefill     --spec-model "meta-llama/Meta-Llama-3.1-8B-Instruct"     --enforce-eager     --enable-chunked-prefill False     --tensor-parallel-size 8     --max_model_len 32832     --input-len 16384     --output-len 1     --batch-size 32     --num-iters-warmup 4     --num-iters 16 > $output_dir/spec_p3_full_lah8_bs32_sl16384.txt

SPEC_CONFIG_PATH=./configs/config_p5_full_lah8.yaml python -m speculative_prefill.vllm_benchmarks.latency     --model "meta-llama/Meta-Llama-3.1-70B-Instruct"     --spec-prefill     --spec-model "meta-llama/Meta-Llama-3.1-8B-Instruct"     --enforce-eager     --enable-chunked-prefill False     --tensor-parallel-size 8     --max_model_len 32832     --input-len 16384     --output-len 1     --batch-size 32     --num-iters-warmup 4     --num-iters 16 > $output_dir/spec_p5_full_lah8_bs32_sl16384.txt

SPEC_CONFIG_PATH=./configs/config_p7_full_lah8.yaml python -m speculative_prefill.vllm_benchmarks.latency     --model "meta-llama/Meta-Llama-3.1-70B-Instruct"     --spec-prefill     --spec-model "meta-llama/Meta-Llama-3.1-8B-Instruct"     --enforce-eager     --enable-chunked-prefill False     --tensor-parallel-size 8     --max_model_len 32832     --input-len 16384     --output-len 1     --batch-size 32     --num-iters-warmup 4     --num-iters 16 > $output_dir/spec_p7_full_lah8_bs32_sl16384.txt

python -m speculative_prefill.vllm_benchmarks.latency     --model "meta-llama/Meta-Llama-3.1-70B-Instruct"     --enforce-eager     --enable-chunked-prefill False     --tensor-parallel-size 8     --max_model_len 32832     --input-len 32768     --output-len 1     --batch-size 16     --num-iters-warmup 4     --num-iters 16 > $output_dir/baseline_bs16_sl32768.txt

SPEC_CONFIG_PATH=./configs/config_p1.yaml python -m speculative_prefill.vllm_benchmarks.latency     --model "meta-llama/Meta-Llama-3.1-70B-Instruct"     --spec-prefill     --spec-model "meta-llama/Meta-Llama-3.1-8B-Instruct"     --enforce-eager     --enable-chunked-prefill False     --tensor-parallel-size 8     --max_model_len 32832     --input-len 32768     --output-len 1     --batch-size 16     --num-iters-warmup 4     --num-iters 16 > $output_dir/spec_p1_bs16_sl32768.txt

SPEC_CONFIG_PATH=./configs/config_p3.yaml python -m speculative_prefill.vllm_benchmarks.latency     --model "meta-llama/Meta-Llama-3.1-70B-Instruct"     --spec-prefill     --spec-model "meta-llama/Meta-Llama-3.1-8B-Instruct"     --enforce-eager     --enable-chunked-prefill False     --tensor-parallel-size 8     --max_model_len 32832     --input-len 32768     --output-len 1     --batch-size 16     --num-iters-warmup 4     --num-iters 16 > $output_dir/spec_p3_bs16_sl32768.txt

SPEC_CONFIG_PATH=./configs/config_p5.yaml python -m speculative_prefill.vllm_benchmarks.latency     --model "meta-llama/Meta-Llama-3.1-70B-Instruct"     --spec-prefill     --spec-model "meta-llama/Meta-Llama-3.1-8B-Instruct"     --enforce-eager     --enable-chunked-prefill False     --tensor-parallel-size 8     --max_model_len 32832     --input-len 32768     --output-len 1     --batch-size 16     --num-iters-warmup 4     --num-iters 16 > $output_dir/spec_p5_bs16_sl32768.txt

SPEC_CONFIG_PATH=./configs/config_p7.yaml python -m speculative_prefill.vllm_benchmarks.latency     --model "meta-llama/Meta-Llama-3.1-70B-Instruct"     --spec-prefill     --spec-model "meta-llama/Meta-Llama-3.1-8B-Instruct"     --enforce-eager     --enable-chunked-prefill False     --tensor-parallel-size 8     --max_model_len 32832     --input-len 32768     --output-len 1     --batch-size 16     --num-iters-warmup 4     --num-iters 16 > $output_dir/spec_p7_bs16_sl32768.txt

SPEC_CONFIG_PATH=./configs/config_p1_full_lah8.yaml python -m speculative_prefill.vllm_benchmarks.latency     --model "meta-llama/Meta-Llama-3.1-70B-Instruct"     --spec-prefill     --spec-model "meta-llama/Meta-Llama-3.1-8B-Instruct"     --enforce-eager     --enable-chunked-prefill False     --tensor-parallel-size 8     --max_model_len 32832     --input-len 32768     --output-len 1     --batch-size 16     --num-iters-warmup 4     --num-iters 16 > $output_dir/spec_p1_full_lah8_bs16_sl32768.txt

SPEC_CONFIG_PATH=./configs/config_p3_full_lah8.yaml python -m speculative_prefill.vllm_benchmarks.latency     --model "meta-llama/Meta-Llama-3.1-70B-Instruct"     --spec-prefill     --spec-model "meta-llama/Meta-Llama-3.1-8B-Instruct"     --enforce-eager     --enable-chunked-prefill False     --tensor-parallel-size 8     --max_model_len 32832     --input-len 32768     --output-len 1     --batch-size 16     --num-iters-warmup 4     --num-iters 16 > $output_dir/spec_p3_full_lah8_bs16_sl32768.txt

SPEC_CONFIG_PATH=./configs/config_p5_full_lah8.yaml python -m speculative_prefill.vllm_benchmarks.latency     --model "meta-llama/Meta-Llama-3.1-70B-Instruct"     --spec-prefill     --spec-model "meta-llama/Meta-Llama-3.1-8B-Instruct"     --enforce-eager     --enable-chunked-prefill False     --tensor-parallel-size 8     --max_model_len 32832     --input-len 32768     --output-len 1     --batch-size 16     --num-iters-warmup 4     --num-iters 16 > $output_dir/spec_p5_full_lah8_bs16_sl32768.txt

SPEC_CONFIG_PATH=./configs/config_p7_full_lah8.yaml python -m speculative_prefill.vllm_benchmarks.latency     --model "meta-llama/Meta-Llama-3.1-70B-Instruct"     --spec-prefill     --spec-model "meta-llama/Meta-Llama-3.1-8B-Instruct"     --enforce-eager     --enable-chunked-prefill False     --tensor-parallel-size 8     --max_model_len 32832     --input-len 32768     --output-len 1     --batch-size 16     --num-iters-warmup 4     --num-iters 16 > $output_dir/spec_p7_full_lah8_bs16_sl32768.txt
